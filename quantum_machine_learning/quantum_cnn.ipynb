{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "comp_device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_dir = os.getcwd()\n",
    "data_dir = os.path.join(project_dir, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 5 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PLOTTING FUNCTIONS\n",
    "\"\"\"\n",
    "\n",
    "def plot_loss_metric(loss, val_loss, metric, val_metric, metric_name='accuracy'):\n",
    "\n",
    "    epochs_range = range(len(loss))\n",
    "\n",
    "    plt.figure(figsize=(20, 10))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    if metric_name == 'accuracy':\n",
    "        plt.plot(epochs_range, metric, label='Training Accuracy')\n",
    "        plt.plot(epochs_range, val_metric, label='Validation Accuracy')\n",
    "        plt.title('Training and Validation Accuracy')\n",
    "    elif metric_name == 'recall':\n",
    "        plt.plot(epochs_range, metric, label='Training Recall')\n",
    "        plt.plot(epochs_range, val_metric, label='Validation Recall')\n",
    "        plt.title('Training and Validation Recall')\n",
    "    elif metric_name == 'precision':\n",
    "        plt.plot(epochs_range, metric, label='Training Precision')\n",
    "        plt.plot(epochs_range, val_metric, label='Validation Precision')\n",
    "        plt.title('Training and Validation Precision')\n",
    "    plt.legend(loc='lower right')\n",
    "    \n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs_range, loss, label='Training Loss')\n",
    "    plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "epochs = 50\n",
    "learning_rate = 1e-3\n",
    "drop_rate = 0.5\n",
    "n_qbits = 9\n",
    "\n",
    "q_device = qml.device(\"default.qubit\", wires=n_qbits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = datasets.MNIST(\n",
    "    root=data_dir,\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "test_data = datasets.MNIST(\n",
    "    root=data_dir,\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(training_data, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "quantum conv\n",
    "\"\"\"\n",
    "\n",
    "class QuantumConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        \n",
    "        weights_tensor = torch.Tensor(self.out_channels, self.in_channels*(self.kernel_size)**2)\n",
    "        self.weights = nn.Parameter(weights_tensor)\n",
    "        nn.init.xavier_uniform_(self.weights)\n",
    "\n",
    "\n",
    "    def compute_windows(self, x):\n",
    "        return nn.functional.unfold(x, self.kernel_size, padding=1)\n",
    "\n",
    "\n",
    "    @qml.qnode(q_device, interface=\"torch\")\n",
    "    def quantum_circuit(x, weights):\n",
    "\n",
    "        weights = torch.unsqueeze(weights, 0)\n",
    "        qml.AngleEmbedding(features=x, wires=range(n_qbits), rotation='X')\n",
    "        qml.BasicEntanglerLayers(weights=weights, wires=range(n_qbits))\n",
    "\n",
    "        return [qml.expval(qml.PauliZ(j)) for j in range(n_qbits)]\n",
    "\n",
    "    \n",
    "    def quantum_conv(self, x, weights):\n",
    "        \n",
    "        measurements = self.quantum_circuit(x, weights)\n",
    "        return torch.mean(measurements)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        padded_input = nn.functional.pad(x, (1,1,1,1), value=0.)\n",
    "        output_tensor = torch.zeros(\n",
    "            [x.shape[0], self.out_channels, 28*28],\n",
    "            dtype = torch.float32\n",
    "        )\n",
    "        \n",
    "        windows = self.compute_windows(padded_input)\n",
    "\n",
    "        for ibatch in range(x.shape[0]):\n",
    "            for chan_out in range(self.out_channels):\n",
    "\n",
    "                for window in range(28*28):\n",
    "                    qconv = self.quantum_conv(windows[ibatch,:,window], self.weights[chan_out])\n",
    "                    output_tensor[ibatch, chan_out, window] = qconv\n",
    "\n",
    "        output_tensor = output_tensor.view(x.shape[0], self.out_channels, 28, 28)\n",
    "        \n",
    "        return output_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.myconv = QuantumConv(1,8)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(28*28*8,64)\n",
    "        self.fc2 = nn.Linear(64,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x = self.myconv(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "customModel = Qnet()\n",
    "opt = torch.optim.Adam(customModel.parameters(), lr=learning_rate)\n",
    "loss_func = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "for param in customModel.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "for child in customModel.children():\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad=True\n",
    "    break\n",
    "\n",
    "for child in customModel.children():\n",
    "    for par in child.parameters():\n",
    "        print(par.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 2.100174  [    0/60000]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\nico_\\OneDrive\\Documenti\\Programmi\\Quantum_computing\\quantum_machine_learning\\quantum_cnn.ipynb Cella 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mt\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m-------------------------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     train(train_dl, customModel, loss_func, opt)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     test(test_dl, customModel, loss_func)\n",
      "\u001b[1;32mc:\\Users\\nico_\\OneDrive\\Documenti\\Programmi\\Quantum_computing\\quantum_machine_learning\\quantum_cnn.ipynb Cella 11\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m size \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(dataloader\u001b[39m.\u001b[39mdataset)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch, (X, y) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloader):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m# Compute prediction and loss\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(X)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     loss \u001b[39m=\u001b[39m loss_fn(pred, y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39m# Backpropagation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\nico_\\OneDrive\\Documenti\\Programmi\\Quantum_computing\\quantum_machine_learning\\quantum_cnn.ipynb Cella 11\u001b[0m in \u001b[0;36mQnet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmyconv(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(x)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflatten(x)\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32mc:\\Users\\nico_\\OneDrive\\Documenti\\Programmi\\Quantum_computing\\quantum_machine_learning\\quantum_cnn.ipynb Cella 11\u001b[0m in \u001b[0;36mQuantumConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mfor\u001b[39;00m chan_out \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_channels):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m         \u001b[39mfor\u001b[39;00m window \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m28\u001b[39m\u001b[39m*\u001b[39m\u001b[39m28\u001b[39m):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m             qconv \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquantum_conv(windows[ibatch,:,window], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweights[chan_out])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m             output_tensor[ibatch, chan_out, window] \u001b[39m=\u001b[39m qconv\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m output_tensor \u001b[39m=\u001b[39m output_tensor\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_channels, \u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\nico_\\OneDrive\\Documenti\\Programmi\\Quantum_computing\\quantum_machine_learning\\quantum_cnn.ipynb Cella 11\u001b[0m in \u001b[0;36mQuantumConv.quantum_conv\u001b[1;34m(self, x, weights)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mquantum_conv\u001b[39m(\u001b[39mself\u001b[39m, x, weights):\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m     measurements \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mquantum_circuit(x, weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/nico_/OneDrive/Documenti/Programmi/Quantum_computing/quantum_machine_learning/quantum_cnn.ipynb#X13sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mmean(measurements)\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pennylane\\qnode.py:661\u001b[0m, in \u001b[0;36mQNode.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    657\u001b[0m             res \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(res)\n\u001b[0;32m    659\u001b[0m     \u001b[39mreturn\u001b[39;00m res\n\u001b[1;32m--> 661\u001b[0m res \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    662\u001b[0m     [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtape],\n\u001b[0;32m    663\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdevice,\n\u001b[0;32m    664\u001b[0m     gradient_fn\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_fn,\n\u001b[0;32m    665\u001b[0m     interface\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterface,\n\u001b[0;32m    666\u001b[0m     gradient_kwargs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgradient_kwargs,\n\u001b[0;32m    667\u001b[0m     override_shots\u001b[39m=\u001b[39;49moverride_shots,\n\u001b[0;32m    668\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute_kwargs,\n\u001b[0;32m    669\u001b[0m )\n\u001b[0;32m    671\u001b[0m \u001b[39mif\u001b[39;00m autograd\u001b[39m.\u001b[39misinstance(res, (\u001b[39mtuple\u001b[39m, \u001b[39mlist\u001b[39m)) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(res) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    672\u001b[0m     \u001b[39m# If a device batch transform was applied, we need to 'unpack'\u001b[39;00m\n\u001b[0;32m    673\u001b[0m     \u001b[39m# the returned tuple/list to a float.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[39m# TODO: find a more explicit way of determining that a batch transform\u001b[39;00m\n\u001b[0;32m    681\u001b[0m     \u001b[39m# was applied.\u001b[39;00m\n\u001b[0;32m    683\u001b[0m     res \u001b[39m=\u001b[39m res[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pennylane\\interfaces\\execution.py:370\u001b[0m, in \u001b[0;36mexecute\u001b[1;34m(tapes, device, gradient_fn, interface, mode, gradient_kwargs, cache, cachesize, max_diff, override_shots, expand_fn, max_expansion, device_batch_transform)\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[39mreturn\u001b[39;00m batch_fn(res)\n\u001b[0;32m    368\u001b[0m \u001b[39mif\u001b[39;00m gradient_fn \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbackprop\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m interface \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    369\u001b[0m     \u001b[39mreturn\u001b[39;00m batch_fn(\n\u001b[1;32m--> 370\u001b[0m         qml\u001b[39m.\u001b[39;49minterfaces\u001b[39m.\u001b[39;49mcache_execute(\n\u001b[0;32m    371\u001b[0m             batch_execute, cache, return_tuple\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, expand_fn\u001b[39m=\u001b[39;49mexpand_fn\n\u001b[0;32m    372\u001b[0m         )(tapes)\n\u001b[0;32m    373\u001b[0m     )\n\u001b[0;32m    375\u001b[0m \u001b[39m# the default execution function is batch_execute\u001b[39;00m\n\u001b[0;32m    376\u001b[0m execute_fn \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39minterfaces\u001b[39m.\u001b[39mcache_execute(batch_execute, cache, expand_fn\u001b[39m=\u001b[39mexpand_fn)\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pennylane\\interfaces\\execution.py:197\u001b[0m, in \u001b[0;36mcache_execute.<locals>.wrapper\u001b[1;34m(tapes, **kwargs)\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[39mreturn\u001b[39;00m (res, []) \u001b[39mif\u001b[39;00m return_tuple \u001b[39melse\u001b[39;00m res\n\u001b[0;32m    195\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[39m# execute all unique tapes that do not exist in the cache\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m     res \u001b[39m=\u001b[39m fn(execution_tapes\u001b[39m.\u001b[39;49mvalues(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    199\u001b[0m final_res \u001b[39m=\u001b[39m []\n\u001b[0;32m    201\u001b[0m \u001b[39mfor\u001b[39;00m i, tape \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tapes):\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pennylane\\interfaces\\execution.py:122\u001b[0m, in \u001b[0;36mcache_execute.<locals>.fn\u001b[1;34m(tapes, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfn\u001b[39m(tapes, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):  \u001b[39m# pylint: disable=function-redefined\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     tapes \u001b[39m=\u001b[39m [expand_fn(tape) \u001b[39mfor\u001b[39;00m tape \u001b[39min\u001b[39;00m tapes]\n\u001b[1;32m--> 122\u001b[0m     \u001b[39mreturn\u001b[39;00m original_fn(tapes, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\contextlib.py:75\u001b[0m, in \u001b[0;36mContextDecorator.__call__.<locals>.inner\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39m@wraps\u001b[39m(func)\n\u001b[0;32m     73\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minner\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m     74\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_recreate_cm():\n\u001b[1;32m---> 75\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pennylane\\_qubit_device.py:586\u001b[0m, in \u001b[0;36mQubitDevice.batch_execute\u001b[1;34m(self, circuits)\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[0;32m    585\u001b[0m     \u001b[39m# TODO: Insert control on value here\u001b[39;00m\n\u001b[1;32m--> 586\u001b[0m     res \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(circuit)\n\u001b[0;32m    587\u001b[0m     results\u001b[39m.\u001b[39mappend(res)\n\u001b[0;32m    589\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtracker\u001b[39m.\u001b[39mactive:\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pennylane\\devices\\default_qubit_torch.py:235\u001b[0m, in \u001b[0;36mDefaultQubitTorch.execute\u001b[1;34m(self, circuit, **kwargs)\u001b[0m\n\u001b[0;32m    226\u001b[0m         \u001b[39mif\u001b[39;00m params_cuda_device \u001b[39m!=\u001b[39m specified_device_cuda:\n\u001b[0;32m    228\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    229\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTorch device \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_torch_device\u001b[39m}\u001b[39;00m\u001b[39m specified \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    230\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mupon PennyLane device creation does not match the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    231\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mTorch device of the gate parameters; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    232\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_torch_device\u001b[39m}\u001b[39;00m\u001b[39m will be used.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    233\u001b[0m             )\n\u001b[1;32m--> 235\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mexecute(circuit, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pennylane\\_qubit_device.py:315\u001b[0m, in \u001b[0;36mQubitDevice.execute\u001b[1;34m(self, circuit, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_validity(circuit\u001b[39m.\u001b[39moperations, circuit\u001b[39m.\u001b[39mobservables)\n\u001b[0;32m    314\u001b[0m \u001b[39m# apply all circuit operations\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply(circuit\u001b[39m.\u001b[39;49moperations, rotations\u001b[39m=\u001b[39;49mcircuit\u001b[39m.\u001b[39;49mdiagonalizing_gates, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    317\u001b[0m \u001b[39m# generate computational basis samples\u001b[39;00m\n\u001b[0;32m    318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshots \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m circuit\u001b[39m.\u001b[39mis_sampled:\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pennylane\\devices\\default_qubit.py:257\u001b[0m, in \u001b[0;36mDefaultQubit.apply\u001b[1;34m(self, operations, rotations, **kwargs)\u001b[0m\n\u001b[0;32m    255\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_debugger\u001b[39m.\u001b[39msnapshots[\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_debugger\u001b[39m.\u001b[39msnapshots)] \u001b[39m=\u001b[39m state_vector\n\u001b[0;32m    256\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 257\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_apply_operation(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_state, operation)\n\u001b[0;32m    259\u001b[0m \u001b[39m# store the pre-rotated state\u001b[39;00m\n\u001b[0;32m    260\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_rotated_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pennylane\\devices\\default_qubit.py:285\u001b[0m, in \u001b[0;36mDefaultQubit._apply_operation\u001b[1;34m(self, state, operation)\u001b[0m\n\u001b[0;32m    282\u001b[0m     axes \u001b[39m=\u001b[39m [ax \u001b[39m+\u001b[39m shift \u001b[39mfor\u001b[39;00m ax \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwires\u001b[39m.\u001b[39mindices(wires)]\n\u001b[0;32m    283\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_ops[operation\u001b[39m.\u001b[39mbase_name](state, axes, inverse\u001b[39m=\u001b[39moperation\u001b[39m.\u001b[39minverse)\n\u001b[1;32m--> 285\u001b[0m matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_asarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_unitary_matrix(operation), dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC_DTYPE)\n\u001b[0;32m    287\u001b[0m \u001b[39mif\u001b[39;00m operation \u001b[39min\u001b[39;00m diagonal_in_z_basis:\n\u001b[0;32m    288\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_apply_diagonal_unitary(state, matrix, wires)\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pennylane\\devices\\default_qubit_torch.py:307\u001b[0m, in \u001b[0;36mDefaultQubitTorch._get_unitary_matrix\u001b[1;34m(self, unitary)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m unitary \u001b[39min\u001b[39;00m diagonal_in_z_basis:\n\u001b[0;32m    306\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_asarray(unitary\u001b[39m.\u001b[39meigvals(), dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC_DTYPE)\n\u001b[1;32m--> 307\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_asarray(unitary\u001b[39m.\u001b[39;49mmatrix(), dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC_DTYPE)\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pennylane\\operation.py:1391\u001b[0m, in \u001b[0;36mOperation.matrix\u001b[1;34m(self, wire_order)\u001b[0m\n\u001b[0;32m   1390\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmatrix\u001b[39m(\u001b[39mself\u001b[39m, wire_order\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m-> 1391\u001b[0m     canonical_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_matrix(\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparameters, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhyperparameters)\n\u001b[0;32m   1393\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minverse:\n\u001b[0;32m   1394\u001b[0m         canonical_matrix \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mconj(qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mmoveaxis(canonical_matrix, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\nico_\\AppData\\Local\\Programs\\Python\\Python38\\lib\\site-packages\\pennylane\\ops\\qubit\\parametric_ops.py:116\u001b[0m, in \u001b[0;36mRX.compute_matrix\u001b[1;34m(theta)\u001b[0m\n\u001b[0;32m    113\u001b[0m     s \u001b[39m=\u001b[39m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mcast_like(s, \u001b[39m1\u001b[39mj)\n\u001b[0;32m    115\u001b[0m \u001b[39m# The following avoids casting an imaginary quantity to reals when backpropagating\u001b[39;00m\n\u001b[1;32m--> 116\u001b[0m c \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m0\u001b[39;49mj) \u001b[39m*\u001b[39;49m c\n\u001b[0;32m    117\u001b[0m js \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39mj \u001b[39m*\u001b[39m s\n\u001b[0;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m qml\u001b[39m.\u001b[39mmath\u001b[39m.\u001b[39mstack([stack_last([c, js]), stack_last([js, c])], axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    \n",
    "    train(train_dl, customModel, loss_func, opt)\n",
    "    test(test_dl, customModel, loss_func)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f84fa9da9fc9effead0dc3a0ddec46df01cd77a4c130e3b9442917017e134d1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
